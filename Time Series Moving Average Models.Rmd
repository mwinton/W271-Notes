---
title: "Time Series Moving Average Models"
author: "Michael Winton"
date: "7/2/2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{amsmath}
geometry: margin=1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4, fig.height=3.5, warn=FALSE)
library(car)
library(dplyr)
library(forecast)
library(stargazer)
```


## Introduction to Moving Average models

A moving average model of order $q$ is a linear combination of the current and past $q$ white noises $w_t$ (aka. "shocks").  Assume that $x_t$ is a demeaned sequence:

$$ x_t = w_t + \beta_1 w_{t-1} + \beta_2 w_{t-2} + ... \beta_{q} w_{t-q}$$

Expressed in backshift operators:

$$ x_t = (1 + \beta_1 B + \beta_2 B^2 + ... + \beta_q B^q) w_t = \phi_q(B)w_t$$

Because MA processes consist of a finite sum of white noise terms, they are stationary with a time-invariant mean, variance, and autocovariance.  In other words, stationarity is met for an MA process, _regardless of its parameters_:

$$ E(x_t) = \sum_{i=0}^q \beta_i E(w_{t-i}) = 0$$
$$ \gamma_0 = Var(x_t) = \sum_{i=0}^q \beta_i^2 Var(w_{t-i}) = (1+\beta_1^2 + ... + \beta_q^2) \sigma_w^2$$
We see that the mean is zero and the variance is a constant.  (Note: we always assume $\beta_0=1$.)  

The autocorrelation function for lag $k \ge 0$, again with $\beta_0=1$:

$$ 
\rho(k) = 
\begin{cases}
  1&,k=0\\
  \big[\sum_{i=0}^{q-k} \beta_i \beta_{i+k} \big]/ \big[ \sum_{i=0}^q \beta_i^2 \big]  &,k=1, ... q\\
  0&,k \ge q
\end{cases}
$$

### Interpreting ACF and PACF plots for MA(q) models

The important takeaway is that the ACF shows that the $MA(q)$ model has a "memory" of only $q$ periods; the current value is _not_ affected by any lag older than $q$ periods.  **We will see the ACF plot drop away suddenly after $q$ periods.**  This happens regardless of the value of the MA $\beta$ parameters.

Also, when we look at the lag $k=1$ line on the ACF plot for MA(1), it will be positive if the $\beta$ parameter is positive and negative if the parameter is negative.

**Unlike the ACF plot, the PACF plot for the MA process will _gradually_ decay to zero.**  This is because of the infinite AR representation of the MA process.  If the $\beta$ parameter is positive, then the PACF will oscillate positive/negative as it decays.  If the parameter is negative, then the PACF will show mostly (but not all) negative spikes.

Higher order MA models have richer dynamics, which can be used to improve forecasting.  However, just based on the plots, it will be hard to distinguish between MA(2) and MA(1) if their first parameters are similar.

Negative $\beta$ parameters in MA models will generate more volatility than positive ones.  Again, the first parameter has a bigger effect than later parameters (due to taking exponents of numbers < 1).

### Invertibility Proof

Now, back to the "shocks"; they are theoretical and _unobservable_.  We cannot use these for forecasting because we don't have an established statistical relationship between current and past values.

The question of whether we can transform a MA model into a form where it can be described as a function of current and past values leads to the concept of _invertibility_.  If the MA process is invertible, it can be transformed into an _infinite order_ AR model that's a function of current "shock" and lagged values of the series. This transformation enables forecasting!

We'll demonstrate the invertibility for an MA(1) process:

$$ x_t = w_t + \beta_1 w_{t-1}$$
Rearranging, we get:
$$ w_t = x_t - \beta_1 w_{t-1}$$
Substituting recursivey, this becomes:
$$ w_t = x_t - \beta_1 (x_{t-1} - \beta_1 w_{t-2}) = x_t - \beta_1 x_{t-1} + \beta_1^2 w_{t-2}$$
If we continue for $n$ steps:
$$ w_t = x_t - \beta_1 x_{t-1} + \beta_1^2 w_{t-2} - \beta_1^3 w_{t-3} + ... + (-\beta_1)^{n+1}w_{t-n-1} $$
Rewriting as a sum:
$$ w_t = \sum_{i=0}^n (- \beta_1)^i x_{t-i} + (- \beta_1)^{n+1}w_{t-n-1}$$
Ultimately this becomes:
$$ w_t = x_t - \beta_1 x_{t-1} + \beta_1^2 w_{t-2} - \beta_1^3 w_{t-3} + ... = \sum_{i=0}^{\infty}(- \beta_1)^i x_{t-i}$$

This can be rearranged to give an equation for $x_t$:
$$ x_t =\beta_1 x_{t-1} - \beta_1^2 w_{t-2} + \beta_1^3 w_{t-3} + ... + w_t = \sum_{i=0}^{\infty}(- \beta_1)^i x_{t-i} + w_t$$
**This is now the _infinite order_ AR model that we can forecast with!  It's only a function of the current white noise term (aka. "shock") and the lagged values in the series!**

An $MA(1)$ process is invertible when all roots $|\beta_1|<1$.  An $MA(2)$ process is invertible if $|\beta_2|<1$ and $|\beta_1| + \beta_2 <1$ (not proven here). 


### MA(1) Simulated Plots

Here are a few examples to look at the effect of different parameters and signs:

```{r}
# MA(1) simulation - effects of positive/negative params
sim1 <- arima.sim(n=100, list(ar=0, ma=c(0.9)))
sim2 <- arima.sim(n=100, list(ar=0, ma=c(0.5)))
sim3 <- arima.sim(n=100, list(ar=0, ma=c(-0.9)))
sim4 <- arima.sim(n=100, list(ar=0, ma=c(-0.5)))
par(mfrow=c(2,2))
plot(sim1, main='MA(1) simulation; c(0.9)')
plot(sim2, main='MA(1) simulation; c(0.5)')
plot(sim3, main='MA(1) simulation; c(-0.9)')
plot(sim4, main='MA(1) simulation; c(-0.5)')
par(mfrow=c(1,1))
```
Interpretation: As described above, negative parameters create more volatility.  Compare the bottom row to the top; also the upper right to upper left. We also note that the magnitude of the parameters (e.g. 0.9 vs. 0.5) doesn't have much effect on the time series.

Next, the ACF plots:
```{r}
# MA(1) simulation - effects of positive/negative params
par(mfrow=c(2,2))
acf(sim1, main='MA(1) simulation; c(0.9)')
acf(sim2, main='MA(1) simulation; c(0.5)')
acf(sim3, main='MA(1) simulation; c(-0.9)')
acf(sim4, main='MA(1) simulation; c(-0.5)')
par(mfrow=c(1,1))
```
Interpretation: We see that they all drop off sharply after the first lag (not the 0th).  Also for lag $k=1$, we see that positive parameters have a positive lag, and vice versa.

Next the PACF plots:
```{r}
# MA(1) simulation - effects of positive/negative params
par(mfrow=c(2,2))
pacf(sim1, main='MA(1) simulation; c(0.9)')
pacf(sim2, main='MA(1) simulation; c(0.5)')
pacf(sim3, main='MA(1) simulation; c(-0.9)')
pacf(sim4, main='MA(1) simulation; c(-0.5)')
par(mfrow=c(1,1))
```
Interpretation: We see that for the positive parameters, the delay oscillates gradually towards zero.  For the negative parameters, the decay is mostly one-sided (negative).

### Comparing MA(1) and MA(2) Simulated Plots

Here are a few examples to look at the effect of different parameters and signs:

```{r}
# MA(1) vs MA(2) simulation - effects of second params
sim1 <- arima.sim(n=100, list(ar=0, ma=c(0.9)))
sim2 <- arima.sim(n=100, list(ar=0, ma=c(0.9, 0.4)))
sim3 <- arima.sim(n=100, list(ar=0, ma=c(0.9, -0.4)))
par(mfrow=c(2,2))
plot(sim1, main='MA(1) simulation; c(0.9)')
plot(sim2, main='MA(2) simulation; c(0.9, 0.4)')
plot(sim3, main='MA(2) simulation; c(0.9, -0.4)')
par(mfrow=c(1,1))
```
Interpretation: The effect of adding a second parameter while keeping the first the same is not easy to distinguish.

### MA(2) Simulated Plots
```{r}
# MA(2) simulation - effects of positive/negative params
sim1 <- arima.sim(n=100, list(ar=0, ma=c(0.9, 0.4)))
sim2 <- arima.sim(n=100, list(ar=0, ma=c(0.9, -0.4)))
sim3 <- arima.sim(n=100, list(ar=0, ma=c(-0.9, 0.4)))
sim4 <- arima.sim(n=100, list(ar=0, ma=c(-0.9, -0.4)))
par(mfrow=c(2,2))
plot(sim1, main='MA(2) simulation; c(0.9, 0.4)')
plot(sim2, main='MA(2) simulation; c(0.9, -0.4)')
plot(sim3, main='MA(2) simulation; c(-0.9, 0.4)')
plot(sim4, main='MA(2) simulation; c(-0.9, -0.4)')
par(mfrow=c(1,1))
```
Interpretation: As described above, negative parameters create more volatility.  Compare the bottom row to the top; also the upper right to upper left.  We also see that the first parameters has more effect than the second.

Now let's look at the ACF plots:
```{r}
par(mfrow=c(2,2))
acf(sim1, main='MA(2) simulation; c(0.9, 0.4)')
acf(sim2, main='MA(2) simulation; c(0.9, -0.4)')
acf(sim3, main='MA(2) simulation; c(-0.9, 0.4)')
acf(sim4, main='MA(2) simulation; c(-0.9, -0.4)')
par(mfrow=c(1,1))
```
Interpretation: TODO
### MA Simulation and Estimation Exercise

Now, run a simulation and then try to estimate it.

```{r}
x3 <- arima.sim(n=1000, list(ar=0, ma=c(0.5, -0.4)))
str(x3)
summary(x3)

par(mfrow=c(2,2))
plot(x3, main='Simulated MA(2) process (0.5, -0.4)')
hist(x3, breaks=20, main='Simulated MA(2) process (0.5, -0.4)')
acf(x3, main='Simulated MA(2) process (0.5, -0.4)')
pacf(x3, main='Simulated MA(2) process (0.5, -0.4)')
par(mfrow=c(1,1))
```

Note that as expected, the ACF plot drops off after lag $k=2$ and the PACF plot gradually decays.  Now let's estimate it using the `arima(p, d, q)` function.  Right now we're only using $q$:

```{r}
(x3_ma <- arima(x3, order=c(0,0,2)))

# take a quick look at the fitted values
summary_df <- data.frame(cbind(x3, fitted(x3_ma), x3_ma$residuals))
head(summary_df)

# concisely display summary statistics with stargazer
stargazer(summary_df, type='text')
```

Now look at diagnostic plots of the residuals:

```{r}
par(mfrow=c(2,2))
plot(x3_ma$residuals, main='Residual Time Series')
hist(x3_ma$residuals, main='Residuals', breaks=20)
acf(x3_ma$residuals, main='ACF of Residuals')
pacf(x3_ma$residuals, main='ACF of Residuals')
par(mfrow=c(1,1))
```

We can also run the **Ljung-Box test** for independence of the residual series.  The test's null hypothesis is of independence.

```{r}
Box.test(x3_ma$residuals, type='Ljung-Box')
```

We cannot reject the null hypothesis of independence.  This is consistent with the ACF and residual time series plots.

Now let's look at the original vs. estimated:

```{r}
# Plot only the last 100 points so that it's easier to see
x3_last100 <- window(x3, start=900, end=1000)
x3_ma_last100 <- window(fitted(x3_ma), start=900, end=1000)
ts.plot(x3_last100, x3_ma_last100,
        gpars = list(col = c("black", "blue"), lty=c(1,2)),
        xlab="Simulated Time Series", ylab='',
        main="Actual vs. Estimated MA(2); params (0.5, -0.4)")
```

Observe that the pointwise fits are not perfect, but we shouldn't expect that; these are only point estimates.  

### Forecasting with MA(q) model

Because of the nature of the $MA(q) model, we can only forecast $q$ steps ahead:

```{r}
forecast(x3_ma,10)
```

We can see that from the $q+1 = 3rd$ step on, the values are constant.