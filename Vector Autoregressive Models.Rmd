---
title: "Vector Autoregressive Models (VAR)"
author: "Michael Winton"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{amsmath}
geometry: margin=1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4, fig.height=3.5, warn=FALSE)

library(car)
library(GGally)
```

## Unit Root Nonstationarity Tests

When investigating a relationship between two time series variables, we need to check whether the two models are nonstationary (ie. have unit roots). If they are, we need to decide whether or not there is a common stochastic trend.  This check for stationarity will be an important step to do before any multivariate time series analysis.

Recall that a characteristic equation of an AR process must have (absolute value of) all roots > 1 in order to be stationarity.  If there is a root of 1 -- a "unit root" -- then that process is nonstationary.  This allows for unit root tests for stationarity.

### Augmented Dickey-Fuller Test

The Augmented Dickey-Fuller (ADF) test is a test for a unit root in a time series.  The null hypothesis is that there is a unit root (ie. nonstationarity).

The original Dickey-Fuller test was of the null hypothesis that $\phi=1$ vs. an alternate hypothesis that $\phi<1$ in the AR(1) model $x_t = x_{t-1} + u_t$ (where $u_t = w_t$ is white noise).  The augmented Dickey-Fuller test expanded to allow $u_t$ to be any stationary process, rather than strictly white noise.
The ADF test approximates that stationary process with an AR model.  The length of the time series determines the power of the ADF test.  In R, we use the `tseries::adf.test` function.

According to the Cowper book, "The null hypothesis of a unit root is favoured by economists because many financial time series are better approximated by random walks than by a stationary process, at least in the short term."
```{r}
library(tseries)
data(tcm)  # built-in data on monthly treasury yields
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
plot(tcm1y, main='Monthly yields of 1-year treasuries')  
acf(tcm1y, main=NA)
pacf(tcm1y, main=NA)
adf.test(tcm1y)
```

The plot shows this data clearly isn't stationary.  The large p-value from the ADF test tells us we do _not_ reject the null hypothesis of unit root (ie. nonstationarity).  A very persistent ACF plot, combined with a PACF plot that quickly drops off sharply are also evidence of a process having a unit root (ie. nonstationary).

### Phillips-Perron Test

This procedure estimates the autocorrelations in the stationary process $u_t$ directly, using a kernel smoother, rather than assuming the AR approximation.  Because of this, it's considered a semi-parametric test.  This shares the same null hypothesis as the ADF test (that there is a unit root, ie. process is non-stationary).  Critical values can be based on asymptotic theory or simulations.  In R, we use `tseries::pp.test`. 

```{r}
pp.test(tcm1y)
```
Again, we fail to reject the null hypothesis of a unit root (ie. non-stationarity).

### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test

In contrast to the other tests, the KPSS test has a null hypothesis of stationarity.

```{r}
kpss.test(tcm1y)
```

With this test, we reject the null hypothesis of stationarity.

## Multiple Time Series - Context

Classical linear regression models assume that stochastic errors are _uncorrelated_.  That assumption is _not_ valid for time series data.  **Two independent time series may actually _appear_ to be correlated to each other, when in reality they are just coincidental, or are both driven by some third factor.  This is called "spurious correlation".**

Don't fall for temptation to fit a regression from one trending time series on another and report high $R^2$.  Common (bad) example: analysts plotting company revenue against macroeconomic time series.

The reason we do unit root tests for nonstationarity is that if two time series are independent and contain unit roots (e.g. they follow independent random walks or at otherwise nonstationary), then we may see an _apparent_ linear relationship, due to _chance similarity_ of the processes over the period of the time series.  That apparent correlation would be spurious.

Is correlation a good measure of the dependency of two time series?  No.  The sample mean is only a good estimator when you can assume data is iid -- not the case for time series data.  Because sample variance, covariance, and correlation calculations use the sample mean, we correlation is not a good measure of dependency between the time series.  High calculated "correlations" may mean nothing, or even worse, could be misleading.

## Cointegration

We can also see cases wherein two individual time series _do have unit roots_, but are actually related.  In other words, they share a common stochastic trend, rather than a spurious correlation.  We refer to these as _cointegrated_.  The formal definition of cointegration is that: two non-stationary time series $x_t, y_t$ are cointegrated if some linear combination of them ($a x_t + b y_t$) is stationary.  

### Random Walk Example

Let's make up a hypothetical example, with a random walk process $\mu_t$:

$$ \mu_t = \mu_{t-1} + w_t$$
Then let's also assume we have two time series:

$$ x_t = \mu_t + w_{x,t}$$

$$ y_t = \mu_t + w_{y,t}$$
where $w_{x,t}$ and $x_{y,t}$ are independent white noise series with zero mean.  These two time series $x_t$ and $y_t$ are both nonstationary.  However, their difference is stationary because it's a linear combination of (stationary) white noise terms:

$$ x_t - y_t = \mu_t + w_{x,t} - \mu_t - w_{y,t} = w_{x,t} - w_{y,t}$$
Hence these two time series are cointegrated because they share the same underlying stochastic trend $\mu_t$.

### Phillips-Ouliaris test for cointegration

The Phillips-Ouliaris (PO) tests the null hypothesis that the two series are _not_ cointegrated. This is implemented in R as `tseries::po.test`. The function requires the series be given in matrix form.

Let's simulate the example given above and test it:

```{r}
x <- y <- mu <- rep(0, 1000) # initialize to zeros
for (t in 2:1000) {
  mu[t] = mu[t-1] + rnorm(1)
}
x <- mu + rnorm(1000)
y <- mu + rnorm(1000)

# look at a few simulated observations
head(cbind(mu,x,y))

# plot the time series
par(mfrow=c(1,1))
ts.plot(x)
lines(y, col='blue', lty='dashed')
lines(mu, col='red', lty='dotted')

# convert data into dataframe to do scatterplot matrix
tmp <- data.frame(x=x,y=y,mu=mu)
ggpairs(tmp)

# perform the ADF test on each:
adf.test(x)$p.value
adf.test(y)$p.value
```

Based on the ADF tests, we cannot reject the null hypothesis of a unit root (nonstationarity).

Let's test stationarity of $x_t - y_t$.  It should be stationary since the difference is just a linear combination of white noise:

```{r}
adf.test(x-y)
```

Based on this test, we reject the null hypothesis of a unit root, providing evidence of stationarity.

Now, check for cointegration:

```{r}
po.test(cbind(x,y))  # need to bind the time series variables into a dataframe
```

The statistically significant p-value in the PO test tells us that we can reject the null hypothesis that they are not cointegrated, thus providing evidence for cointegration.

## Vector AR Models

In VAR models, all variables are treated symmetrically, ie. they affect each other.  We don't limit ourselves to a unidirectional relationship where predictors influence the forecast variable.  For example, increased income leads to increased spending, which leads to increased income, etc...   In the VAR model, all variables are modeled as if they influence each other equally.  If you put a lag on a particular variable in one equation, you need to put it in every equation. We call them _endogenous_.  

**All of the time series have to be stationary because it's an extension of an AR model (VAR in level).**  If series are not stationary, you may need to take differences to transform it into a stationary series (VAR in difference).  For a cointegrated series (where a difference of two equations is stationary), there's something called VECM (Vector Error Correction Model).  Vector ARMA model sounds theoretically possible, but in reality is near impossible to identify empirically due to the large number of parameters.

### VAR Mathematical Model

Let's look at a simple case of a VAR(1) model ($w_{x,t}$ and $w_{y,t}$ are _bivariate_ white noise):

$$ x_t = \phi_{11} x_{t-1} + \phi_{12} y_{t-1} + w_{x,t}$$
$$ y_t = \phi_{21} x_{t-1} + \phi_{22} y_{t-1} + w_{y,t}$$
We see from the $t$ subscripts on the white noise terms that they are contemporaneously correlated.  In vector form, this can be written as:

$$ \bf{Z_t = \Phi Z_{t-1} + w_t}$$
where $\bf{Z_t} = {{x_t} \choose{y_t}}$, $\bf{\Phi} = {{\phi_{11}\ \phi_{12}} \choose {\phi_{21}\ \phi_{22}}}$ and $\bf{w_t} = {w_{x,t} \choose w_{y,t}}$.

This can also be expressed in terms of the backshift operator:
$$[I -\Phi(B)] Z_t =  w_t$$

Stationarity of a $AR(P)$ model is defined similarly to an $AR(p)$ model: roots of the characteristic equation must all be outside the unit circle.  The characteristic equation is the _determinant_ of the $\bf{I -\Phi( } B \bf{)}$ matrix.  This can be solved algebraically, but in R we use `Mod(polyroot(...))` or `abs(polyroot(...))` (equivalent). 

For example, if we have the following parameter matrix:

$$
\Phi(B) =\left[ {\begin{array}{cc}
0.4 & 0.3 \\
0.2 & 0.1 \\
\end{array} } \right]
$$

The characteristic polynomial is:
$$
I - \Phi(B) =\left[ {\begin{array}{cc}
1-0.4x & 0.3x \\
0.2x & 1-0.1x \\
\end{array} } \right]
$$
Now, taking the determinant, we get this polynomial:
$$
= (1-0.4x)(1-0.1x)-(0.3x)(0.2x) = 1 - 0.4x - 0.1x + 0.04x^2 - 0.06x^2 = 1 -0.5x - 0.02 x^2
$$
Plugging in to R to solve for the roots, we get:

```{r}
# these two are equivalent
Mod(polyroot(c(1,-0.5,-0.02)))
abs(polyroot(c(1,-0.5,-0.02)))
```

Since the absolute value of both roots are > 1, we know that this VAR(1) model is stationary.

### Simulating and estimating a VAR(1) model

Let's simulate two time series with these same parameters.  First we have to simulate a multivariate white noise process for $w_{x,t}$ and $w_{y,t}$.

```{r}
# first simulate multivariate white noise
library(mvtnorm)
cov_mat <- matrix(c(1, 0.8, 0.8, 1), nr=2)
w <- rmvnorm(1000, sigma=cov_mat)

# observe what the simulated multivariate white noiselooks like
head(w)
tail(w)
plot(w)

# estimated covariance should be similar
cov(w)

# create our two multivariate white noise params
wx <- w[,1]
wy <- w[,2]
head(wx)
head(wy)

# check cross-correlation
ccf(wx, wy)
```

The `ccf` function verifies that cross-correlations are ~0 for all non-zero lags, which is required in the definition of bivariate white noise.  They are contemporaneously correlated, but do not form a lag-lead relationship.

Now we'll simulate a VAR(1) process:

```{r}
x <- y <- rep(0, 1000)  # initialize to zeros
x[1] <- wx[1]
y[1] <- wy[1]
for (t in 2:1000) {
  x[t] <- 0.4 * x[t-1] + 0.3 * y[t-1] + wx[t]
  y[t] <- 0.2 * x[t-1] + 0.1 * y[t-1] + wy[t]
}

# look at ccf:
ccf(x,y, main='Cross-correlation function for a VAR(1) process')
```

We see the expected cross-correlation of a VAR(1) model.

```{r}
# plot the scatterplot matrix
ggpairs(data.frame(cbind(x,y)))

# plot the time series
ts.plot(x)
lines(y, col='blue', lty='dashed')

# plot the ACF, PACF
par(mfrow=c(2,2))
acf(x)
acf(y)
pacf(x)
pacf(y)
```

We see the correlation is close to the theoretical (based on simulation parameters).  Now let's estimate a model with `ar`:

```{r}
(xy_ar <- ar(cbind(x,y)))
# xy_ar$ar[,,]
```

We see that the estimated parameters are reasonably close to the estimated (however, variances are very high).  

### Bootstrap Confidence Intervals

From Cowper:

"If the simulation is repeated many times with different realisations of the bivariate white noise, the sampling distribution of the estimators of the parameters in the model can be approximated by the histograms of the esti- mates together with the correlations between estimates. This is the principle used to construct bootstrap confidence intervals for model parameters when they have been estimated from time series."
    
"The bootstrap simulation is set up using point estimates of the parameters in the model, including the variance of the white noise terms. Then time series of the same length as the historical records are simulated and the parameters estimated. A $(1−\alpha) × 100%$ confidence interval for a parameter is between the lower and upper $\alpha/2$ quantiles of the empirical sampling distribution of its estimates."

## Example: UK and EU Exchange Rates

Load data (from Cowper) and format as time series and look at "correlation"

```{r}
xrates <- read.table('us_rates.dat', header=TRUE)
head(xrates)
UK_ts <- ts(xrates$UK)
NZ_ts <- ts(xrates$NZ)
EU_ts <- ts(xrates$EU)

# plot data
ts.plot(UK_ts, NZ_ts, EU_ts, col=c('black','blue','red'))

# we're not surprised to see correlation
plot(EU_ts ~ UK_ts)

# check for unit roots
adf.test(EU_ts)
adf.test(UK_ts)

# check for cointegration
po.test(cbind(UK_ts, EU_ts))
```

We fail to reject the null hypothesis of the ADF tests that the series have unit roots (ie. are nonstationary).  The statistically significant p-value in the PO test provides evidence of cointegration.  Since they are cointegrated, we can fit a linear regression of EU on UK:

```{r}
EU_UK_mod <- lm(UK_ts ~ EU_ts)
summary(EU_UK_mod)
residualPlots(EU_UK_mod)
```

Not surprisingly, the slope coefficient is highly significant, and the $R^2$ is very high.  They are quite strongly related.  We see a clear pattern in both residuals plots.  Now, we'll model the residuals as a stationary time series.  Start with the typical EDA plots:

```{r}
EU_UK_r <- resid(EU_UK_mod)
par(mfrow=c(2,2))
plot(EU_UK_r, main='Residuals from linear regression of UK Pound on Euro', type='l')
plot(density(EU_UK_r))  #  can plot density as an alternative to histogram
acf(EU_UK_r)
pacf(EU_UK_r)
```

If EU and UK data are cointegrated, we can find a linear relationship such that there's a stationary time series.  The EDA plots indicate an AR(1) model may be appropriate.   Let's use the `ar` function (which decides based on AIC).

```{r}
(EU_UK_r_mod <- ar(EU_UK_r))

# do diagnostics on the residuals from this model
mod_resids <- na.omit(EU_UK_r_mod$resid)
plot(mod_resids)
qqnorm(mod_resids)
acf(mod_resids)
pacf(mod_resids)
```

The diagnostics look pretty much like white noise, although we'd rather the PACF not show any significant lags.

## Example 2: Cardiovascular Mortality, Temp, and Particulate Matter

```{r include=FALSE}
library(astsa)
library(vars)
```

Load data from `astsa` package.  For this example, we'll use the `vars` package, too.

```{r}
cmtp <- cbind(cmort, tempr, part)
head(cmtp)
par(mfrow=c(1,1))
plot.ts(cmtp)  # gives a nicer display than ts.plot (which overlays them)

# look at correlation between the time series
ggpairs(data.frame(cmtp))
ccf(cmort, tempr)
ccf(cmort, part)
ccf(tempr, part)

# plot ACF and PACF plots
par(mfrow=c(3,2))
acf(cmort)
pacf(cmort)
acf(tempr)
pacf(tempr)
acf(part)
pacf(part)
par(mfrow=c(1,1))
```

We see they all have strong seasonality.  Temperature and particulates don't seem particularly contemporaneously correlated (based on scatterplot); the others are more so.  All of the cross-correlation plots show strong lag-lead cross-correlation, too.    ACF plots all show strong persistence.  PACF plots also show quite a few significant lags.

Fit a VAR(1) model for the sake of a demo (not because EDA suggests it):

```{r}
summary(VAR(cmtp, p=1, type='both'))  # 'both' means both constant (drift) and trend
```

Review the summary to see lots of interpretations about relationships between the time series, roots of characteristic equations, etc......

Now, let `var` package fit models with multiple values for $p$, since we had no particular reason to pick VAR(1) before.  Note that $SC(n) = BIC$.

```{r}
VARselect(cmtp, lag.max=10, type='both')
```

Let's look at the results of the $VAR(2)$ model, recommended by BIC.  Parsimonious models are usually better for forecasting.

```{r}
summary(var2_fit <- VAR(cmtp, p=2, type='both'))
```

And do the diagnostics on the residuals:

```{r}
par(mfrow=c(2,2))
ts.plot(resid(var2_fit))
qqnorm(resid(var2_fit))
acf(resid(var2_fit), lag.max=52)
pacf(resid(var2_fit), lag.max=52)
```

We can also test the serial correlation between the residuals with the Portmanteau Test.  Its null hypothesis is that there is no autocorrelation (or serial correlation) among the residuals.  The alternate hypothesis is that at least one lag has a non-zero coefficient (and thus there's autocorrelation).  The Ljung-Box test we performed on ARIMA models using `box.test` is also a type of Portmanteau test.  However, we use `serial.test` for VAR models.

```{r}
serial.test(var2_fit, lags.pt=12, type = 'PT.adjusted')
```

The highly statistically significant p-value tells us that we strongly reject the null hypothesis (of independence / no correlation between residuals).

Now, do 4-week and 8-week forecasts with this model for the sake of demonstration:

```{r}
fcast <- predict(var2_fit, n.ahead=24, ci=0.95)  # 4 weeks ahead
fanchart(fcast)  # plot prediction and error

fcast <- predict(var2_fit, n.ahead=48, ci=0.95)  # 8 weeks ahead
fanchart(fcast)  # plot prediction and error
```
