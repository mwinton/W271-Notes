---
title: "Time Series Regression and Smoothing"
author: "Michael Winton"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{amsmath}
geometry: margin=1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4, fig.height=3.5)
library(car)
library(dplyr)
```

## Linear Trend Time Regression

A classical linear regression would take the following form for the conditional mean:

$$ y_t = \beta_0 + \beta_1 x_{t1} + \beta_2 x_{t2} + ... + \beta_k x_{tk} + \epsilon_t$$
The $\beta$ terms are the unknown regression parameters, and the $\epsilon_t$ term represents a stochastic process with random variables with $mean=0$ and $variance=\sigma_{\epsilon}^2$.  If we assume the random variables follow a normal distribution, then this is the Classical Normal Linear Regression Model.

Use global temperature dataset as an example:
```{r}
# load dataset
url <- 'https://raw.githubusercontent.com/mwinton/Introductory_Time_Series_with_R_datasets/master/global.dat'
gtemp_df <- scan(url)
gtemp_ts <- ts(gtemp_df, start=c(1856, 1), end=c(2005, 12), freq=12)
str(gtemp_ts)

gtemp_annual <- aggregate(gtemp_ts, FUN=mean)
str(gtemp_annual)
gtemp_annual <- window(gtemp_annual, c(1880))  # to more closely match async
str(gtemp_annual)

# plot time series
plot(gtemp_annual, xlab="Year", ylab="Temp Deviation", main="Global Temperature Rise")
```

Observe that the temperature started to trend upwards around 1920.  We can consider the following linear time trend regression model:  
$$ y_t = \beta_0 + \beta_1 x_{t1} +\epsilon_t$$

We can replace $t=1856, 1857, ...$ with $t=1, 2, ...$ without changing $\beta_1$; only the intercept changes. This model assumes that $\epsilon_t$ is an iid normal sequence, but we will _need to verify this later_.  To estimate this regression:

```{r}
gtemp_lt <- lm(gtemp_annual ~ time(gtemp_annual))
summary(gtemp_lt)
plot(gtemp_annual, xlab="Year", ylab="Temp Deviation", main="Global Temperature Rise")
abline(gtemp_lt, lty=2)
```

Visually, we see that this model doesn't do a very good job of capturing the pattern in our data, despite the relatively high Adjusted $R^2$.  Performing regression diagnostics:

```{r}
par(mfrow=c(2,2))
plot(gtemp_lt)
par(mfrow=c(1,1))

# test for heteroskedasticity (H_0: homoskedasticity)
library(lmtest)
bptest(gtemp_lt)
```
We see in particular from the Residuals vs Fitted plot that we very clearly violate the zero conditional mean assumption.  The plots show that may be some minor violation of the homoskedasticity assumption, but the Breusch Pagan test fails to reject the null hypothesis of homoskedasticity.  Also, residuals are normally distributed.

### Goodness of Fit (AIC, AICc, BIC)

These are commonly used _in-sample_ measures of goodness of fit.  (We desire lower values of the IC.)  These criteria all penalize the use of extra parameters in the model.  BIC is the most stringent of these.

Akaike's Information Criterion:
$$ AIC = log (\hat{\sigma}_k^2) + \frac{n+2k}{n}$$

where $\hat{\sigma}_k^2 = SSE_k/n$ and $SSE_k/n = \sum_{t=1}^n(x_t - \hat{\beta}' z_t)^2$.  $k$ is the number of parameters in the model, and $n$ is the sample size.

Bias-Corrected AIC:
$$ AIC_c = log (\hat{\sigma}_k^2) + \frac{n+k}{n-k-2}$$

Bayesian's Information Criterion:
$$ BIC = log (\hat{\sigma}_k^2) + \frac{k \cdot log(n)}{n}$$
### Regressing one time series on another

Sometimes we want to use one time series to forecast the value of another.  However, we can't use future to forecast future, so we need to use a lag in our model.

When you are working with two time series, you need to do EDA on both, and also plot correlation between them.

The (ASTSA) textbook tells us that lag values of Southern Oscillation Index are correlated with the current value of Recruitment.  We could entertain a simple model based on a 6 month lag.  This would allow us to use today's SOI value to forecast the value of Recruitment 6 months in the future.

$$ R_t = \beta_1 + \beta_2 S_{t-6} + w_t$$

The `ts.intesect(...)` helps us do time series alignment.  
```{r}
# SOI and Recruitment data is in astsa library
library(astsa)
soi_ts <- soi
str(soi_ts)
rec_ts <- rec
str(rec_ts)

# make sure to do time index alignment; new ts object includes both ts's
fish <- ts.intersect(rec_ts, soi_l6=stats::lag(soi_ts, -6))
str(fish)

# build regression model
fit2 <- lm(rec_ts ~ soi_l6, data=fish, na.action=NULL)
summary(fit2)
```

## Smoothing Techniques

Smoothing techniques are often used to under trend and cyclical components of a time series.  THe general concept is to take a weighted average of values in a moving window.

### Moving (Rolling) Averages

A _symmetric_ moving average smoother takes the form:

$$ m_t = \sum_{j=-k}^k a_j x_{t-j}$$

where $a_j \ge 0$ and sum of the weights $\sum_{j=-k}^k a_j =1$.

For example, setting $k=2$ with weekly data essentially results in a monthly series, and can help bring out a seasonality pattern:

$$ m_t = \frac{1}{5} \sum_{j=-2}^2  x_{t-j} = \frac{1}{5}[x_{t-2} + x_{t-1} + x_{t} + x_{t+1} + x_{t+2}]$$
Similarly, setting $k=26$ would essentially give a yearly series (with $a=\frac{1}{53}$).  Note that some people only use backward smoothing techniques so we don't have to rely on "future" data for each point.

### Polynomial and Periodic Regression Smoothers

This class of smoothing technique requires that we define a smoothing function $f_t$ and a $z_t$ stationary process.

$$ x_t = f_t + z_t$$
We can use a polynomial function for $f_t$ (where we specify the degree of polynomial $p$):
$$ f_t = \sum_{i=0}^p \beta_i t^i$$
For periodic data, we use a periodic function such as:

$$ f_t = \sum_{i=0}^p \alpha_i cos(2\pi w_i t) \beta_i sin(2\pi w_i t)$$
where $cos(2\pi w_0 t) = sin(2\pi w_0 t) = 1$ and $w_1, w_2, ...w_p$ are distinct, specified frequencies.

The polynomial and periodic polynomial functions can be combined as one smoother in a classical linear regression.

### Spline Smoothers

This is an extension of the polynomial smoothing technique by dividing the modeling time horizon into $k$ mutually exclusive and exhaustive intervals, and then fitting a polynomial regression to each of the intervals.

$$[t_0=1,t_1], [t_1+1, t_2],...,[t_{k-1}+1, t_k=n]$$
where $t_0, t_1, ... t_k$ are called the "knots."  Each interval is fit with a regression of the form:

$$ f_t = \beta_0 + \beta_1 t + ... \beta_p t^p$$
A common choice is $p=3$ for a cubic spline.

The _smoothing splines_ technique modifies the spline technique by incorporating the penalized smoothness component in the objective function such that the minimization problem accounts for the tradeoff between model fit and degree of smoothness.    The objective function is written (with smoothing parameter $\lambda$, and $f_t$ is a cubic spline) as: 
$$ \sum_{t=1}^n (x_t - f_t)^2 + \lambda \int(f_t'')^2dt$$

### Kernel Smoothers

A kernel smoother is a symmetric moving average smoother with a probability density weight function.

$$ \hat{f}_t = \sum_{i=1}^n w_i(t)x_i \text{  where  } w_i(t)=\frac{K(\frac{t-i}{b})}{\sum_{j=1}^n K(\frac{t-i}{b})}$$

### Exponential Smoothers

We assume no systematic trend or seasonality, and that the mean of a process can change from step to step (but we don't know which direction).  Example: forecasting sales of a well-established product in a stable market.  The model is:

$$ x_t = \mu_t + w_t$$
Where $x_t$ is our observation at time $t$, $\mu_t$ is the nonstationary mean, and $w_t$ is the white noise with $mean=0$ and $variance=\sigma_w^2$.  A reasonable estimate of the mean at time $t$ would be a weighted average of our observation at time $t$ and our estimate of the mean at $t-1$:

$$ \hat{\mu}_t = \alpha x_t + (1-\alpha)\hat{\mu}_{t-1} \text{ where } 0 < \alpha < 1$$

We refer to $\hat{\mu}$ as the _exponentially weighted moving average (EWMA)_ at $t$ and $\alpha$ as the smoothing parameter. $\alpha$ near 0 is heavily smoothed; near 1 is minimally smoothed.  We would only use $\alpha$ near one if we expect changes in the mean to be large compared to $\sigma$.  A typical value is $\alpha=0.2$ because we usually expect the change between $t-1$ and $t$ to be less than one standard deviation.  

NOTE: R will calculate $\alpha$ if you don't provide it, by minimizing sum of squared one-step-ahead forecast errors.  This is not good for long series with means that change little, because it makes $\alpha$ too small.

We can also rewrite the equation in terms of one-step-ahead forecast error ($x_t - \hat{\mu}_{t-1}$):

$$ \hat{\mu}_t = \alpha (x_t - \hat{\mu}_{t-1}) + \hat{\mu}_{t-1} \text{, with starting value }\hat{\mu}_1=x_1$$
Also, by repeated backpropagation:

$$ \hat{\mu}_t = \alpha x_t + \alpha(1-\alpha)x_{t-1} + \alpha(1-\alpha)^2x_{t-2} + ...$$
This is a combination of the current observation and all past observations, with more weight given to the most recent ones.  Note the weights form a geometric series, and the sum of the infinite series is 1.



## Worked Example with Various Smoothers

### EDA
First we load and do a quick EDA on the US first time unemployment claim data.
```{r}
unemployment_df <- read.csv('jobless-claims-historical-chart.csv', skip=13, header=TRUE)
unemployment_df <- unemployment_df %>% filter(value>0) %>% mutate(thousands=value/1000)
unemployment_ts <- ts(unemployment_df$thousands, start=c(1967,1), freq=12)

# EDA
head(unemployment_ts)
str(unemployment_ts)
par(mfrow=c(2,2))
plot(unemployment_ts, xlab="Monthly Series", ylab="First Time Claims (in Thousands)",
     main="US Initial Unemployment Claims")
hist(unemployment_ts, xlab="First Time Claims (in Thousands)",
     main="US Initial Unemployment Claims")
acf(unemployment_ts, main="ACF of US Initial Unemployment Claims")
pacf(unemployment_ts, main="Partial ACF of US Initial Unemployment Claims")
```

Now we will show some smoothing techniques.  

### Moving Averages

First, we use `filter(...)` to calculate moving averages.

```{r}
# calculate moving averages
# explicitly calling stats::filter so we don't get dplyr::filter
ma3 <- stats::filter(unemployment_df$thousands, rep(1,3)/3, sides=2)  # 3 months
ma40 <- stats::filter(unemployment_df$thousands, sides=2, rep(1,40)/40)  # 40 months

# convert the moving averages to ts objects
ma3_ts <- ts(ma3, start=c(1967,1), freq=12)
ma40_ts <- ts(ma40, start=c(1967,1), freq=12)

# use ts.plot to overlay multiple time series
par(mfrow=c(1,1))
ts.plot(unemployment_ts, ma3_ts, ma40_ts, gpars = list(col = c("black", "red", "blue")),
        xlab="Monthly Series", ylab="First Time Claims (in Thousands)",
        main="US Initial Unemployment Claims (Moving Average Smoother)")
```

The smoother that only uses 3 observations almost mirrors the original data. The one with 40 observations doesn't follow as drastically; it pulls the curve back down away from the peaks.

### Polynomial Regression Smoother

Now we'll try a regression smoother with a 3rd degree polynomial.

```{r}
# fit a polynomial regression
mo <- time(unemployment_ts) - mean(time(unemployment_ts))
mo_squared <- mo^2
mo_cubed <- mo^3
reg_smoother <- lm(unemployment_ts ~ mo + mo_squared + mo_cubed, na.action=NULL)

# plot the individual components and the regression
# par(mfrow=c(2,2))
# plot(mo)
# plot(mo_squared)
# plot(mo_cubed)
# par(mfrow=c(1,1))
plot(unemployment_ts, type="l", main="Polynomial Regression Smoother")
lines(fitted(reg_smoother), col="red")
```

This smoother is not a particularly good choice for this dataset.  It does not capture the trend dynamics well for the last 10-15 years.

### Spline Smoother

We let R calculate the splines.  We have to play around with the `spar` parameter.

```{r}
plot(unemployment_ts, type="l", main="Spline Smoother")
lines(smooth.spline(time(unemployment_ts), unemployment_ts, spar=0), col="green")
lines(smooth.spline(time(unemployment_ts), unemployment_ts, spar=0.5), col="blue")
lines(smooth.spline(time(unemployment_ts), unemployment_ts, spar=1), col="purple")
lines(smooth.spline(time(unemployment_ts), unemployment_ts, spar=2), col="red")
```

The purplse and red smoothers don't capture enough of the pattern of the data.

### Kernel Smoother

Kernel smoothing can usually capture the underlying trend very smoothly.  We have to play around with the kernel function (e.g. "normal") and with the bandwidth (how wide is the window being used for the calculation).

```{r}
plot(unemployment_ts, type="l", main="Kernel Smoother")
lines(ksmooth(time(unemployment_ts), unemployment_ts, "normal", bandwidth=0.5), col="green")
lines(ksmooth(time(unemployment_ts), unemployment_ts, "normal", bandwidth=1), col="blue")
lines(ksmooth(time(unemployment_ts), unemployment_ts, "normal", bandwidth=2), col="purple")
lines(ksmooth(time(unemployment_ts), unemployment_ts, "normal", bandwidth=5), col="red")
```

### Exponential Smoother

We use a time series without any clear trend or seasonality for this example:
```{r}
url <- 'https://raw.githubusercontent.com/mwinton/Introductory_Time_Series_with_R_datasets/master/motororg.dat'
complaints_df <- read.table(url, header=TRUE)
complaints_ts <- ts(complaints_df$complaints, start=c(1996, 1), frequency=12)
par(mfrow=c(1,1))
# need to set beta, gamma=FALSE for exponential smoothing
(exp_smooth <- HoltWinters(complaints_ts, alpha=0.2, beta=FALSE, gamma=FALSE))
# plot command plots the data and smoother
plot(exp_smooth, main="Exponential Smoothing (alpha=0.2)")
```


## Eliminating the Trend

1. Estimate a (linear) trend for the series
2. Detrend the series - subtract the trend from the observed series
3. Model residuals using a stationary model

Again, we'll work with the global temperature data.

```{r}
# use the dataset from astsa
library(astsa)
str(gtemp)
time(gtemp)  # good summary to make sure we know the frequency of the data
# plot time series
plot(gtemp, type="o", xlab="Year", ylab="Temp Deviation", main="Global Temperature Rise")
```

First, estimate the linear trend:

```{r}
# 1. estimate linear trend
gtemp_lt <- lm(gtemp ~ time(gtemp), na.action=NULL)
summary(gtemp_lt)
plot(gtemp, type="o", xlab="Year", ylab="Temp Deviation", main="Global Temperature Rise")
abline(gtemp_lt)
```

Then we detrend the series; plotting the residuals shows the results:

```{r}
# 2. detrend the series
plot(resid(gtemp_lt), type="o", main="Detrend Global Temp Series")
```

This did a pretty good for the majority of the series.  (Not quite as good at the right; polynomial fits might do a better job detrending this dataset.)  If we desire to model the stationary series, we model these residuals with a stationary model.  We also will need to establish whether the stationarity assumption is valid.  

### First Differencing

Note that there's also another approach we could take: _First Differencing_.  We want to "undifference" the series when we want to do forecasting.

```{r}
plot(diff(gtemp), type="o", main="First Difference of the Global Temperature Series")
```

